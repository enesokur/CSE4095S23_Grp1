{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1cMDumDwC2a22jBWB30Mtll0BusovpWVV","authorship_tag":"ABX9TyMcx+mH2mpmRVMGgEuP2pI3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**WARNING: Before you run the project, you should run nltk.download(\"all\") code in order to download nltk package. Training usually takes between 10-12 minutes.**"],"metadata":{"id":"UNyCh4TL-dAH"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"all\")"],"metadata":{"id":"rmf3LGy--zuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzVqPvf9GMve"},"outputs":[],"source":["import json\n","import os\n","import nltk\n","import random\n","from nltk.util import ngrams\n","from nltk.lm import Laplace\n","from nltk.lm.preprocessing import padded_everygram_pipeline\n","\n","# Reading json files from file. This file was in my google drive. Make sure file name is correct and json files are in there.\n","dir = \"/content/drive/MyDrive/2021-01-json-9k\"\n","jsons = []\n","for filename in os.listdir(dir):\n","  if filename.endswith(\".json\"):\n","    jsonpath = os.path.join(dir, filename)\n","    with open(jsonpath, \"r\") as jsonfile:\n","            myjsonfile = json.load(jsonfile)\n","            jsons.append(myjsonfile)\n","tokens_no_stop_words = []\n","stop_words = nltk.corpus.stopwords.words('turkish')\n","chunk = [\".\",\",\",\"/\",\"[\",\"]\",\":\",\";\",\"...\"]\n","\n","#Reading words from json files and tokenizing them. Then removing expressions like \".\" , \" /\" etc and stop words.\n","for json in jsons:\n","  temptext = \"\"\n","  for i in json[\"vurgular\"]:\n","    temptext += i + \" \"\n","  temptext += json[\"Mahkemesi\"] + \" \" + json[\"Dairesi\"] + \" \" + json[\"Esas\"] + \" \" + json[\"Birinci Mahkemesi\"] + \" \" + json[\"Karar\"] + \" \" + json[\"Mahkeme Günü\"] + \" \" + json[\"Mahkeme Ayı\"] + \" \" + json[\"Mahkeme Yılı\"] + \" \" + json[\"Suç\"] + \" \" + json[\"Hüküm\"] + \" \" + json[\"Dava Türü\"] + \" \" + json[\"ictihat\"] + \" \"\n","  temptext = temptext.lower()\n","  tokens = nltk.word_tokenize(temptext)\n","  for i in tokens:\n","      if i in chunk:\n","        tokens.remove(i)\n","  for i in tokens:\n","     if i not in stop_words:\n","        tokens_no_stop_words.append(i)\n","\n","# Splitting words into test and training sets for word based n-gram model.\n","training_size_for_word_based = int(len(tokens_no_stop_words) * 0.8)\n","training_set_for_word_based = tokens_no_stop_words[:training_size_for_word_based]\n","test_set_for_word_based = tokens_no_stop_words[training_size_for_word_based:]\n","\n","#Forming 2-grams and training model.\n","training_ngrams_for_word_based, vocab = padded_everygram_pipeline(2, training_set_for_word_based)\n","word_based_model = Laplace(1)\n","word_based_model.fit(training_ngrams_for_word_based,vocab)\n","\n","#Testing word based n-gram model\n","test_ngrams_for_word_based = ngrams(test_set_for_word_based,2)\n","perplexity = word_based_model.perplexity(test_ngrams_for_word_based)\n","print(\"Perplexity for word based model: \", perplexity)\n","\n","# Tokenizing words into characters.\n","characters_from_tokenized_no_stop_words = []\n","for i in tokens_no_stop_words:\n","  for j in i:\n","    characters_from_tokenized_no_stop_words.extend(j)\n","\n","#Splitting characters into training and test sets.\n","training_size_for_character_based = int(len(characters_from_tokenized_no_stop_words) * 0.8)\n","training_set_for_character_based = characters_from_tokenized_no_stop_words[:training_size_for_character_based]\n","test_set_for_character_based = characters_from_tokenized_no_stop_words[training_size_for_character_based:]\n","\n","# Forming 2-grams for character based model and training it.\n","training_ngrams_for_character_based, vocab2 = padded_everygram_pipeline(2,training_set_for_character_based)\n","character_based_model = Laplace(1)\n","character_based_model.fit(training_ngrams_for_character_based,vocab2)\n","\n","# Testing character based model.\n","test_ngrams_for_character_based = ngrams(test_set_for_character_based,2)\n","perplexity = word_based_model.perplexity(test_ngrams_for_character_based)\n","print(\"Perplexity for character based model: \", perplexity)\n","\n","\n","\n","\n","\n"]}]}