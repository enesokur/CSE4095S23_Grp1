{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**WARNING: Before you run the project, you should run nltk.download(\"all\") code in order to download nltk package. Training usually takes between 10-12 minutes.**"
      ],
      "metadata": {
        "id": "UNyCh4TL-dAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "id": "rmf3LGy--zuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzVqPvf9GMve"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import random\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "# Reading json files from file. This file was in my google drive. Make sure file name is correct and json files are in there.\n",
        "dir = \"/content/drive/MyDrive/2021-01-json-9k\"\n",
        "jsons = []\n",
        "for filename in os.listdir(dir):\n",
        "  if filename.endswith(\".json\"):\n",
        "    jsonpath = os.path.join(dir, filename)\n",
        "    with open(jsonpath, \"r\") as jsonfile:\n",
        "            myjsonfile = json.load(jsonfile)\n",
        "            jsons.append(myjsonfile)\n",
        "tokens_no_stop_words = []\n",
        "stop_words = nltk.corpus.stopwords.words('turkish')\n",
        "chunk = [\".\",\",\",\"/\",\"[\",\"]\",\":\",\";\",\"...\"]\n",
        "\n",
        "#Reading words from json files and tokenizing them. Then removing expressions like \".\" , \" /\" etc and stop words.\n",
        "for json in jsons:\n",
        "  temptext = \"\"\n",
        "  for i in json[\"vurgular\"]:\n",
        "    temptext += i + \" \"\n",
        "  temptext += json[\"Mahkemesi\"] + \" \" + json[\"Dairesi\"] + \" \" + json[\"Esas\"] + \" \" + json[\"Birinci Mahkemesi\"] + \" \" + json[\"Karar\"] + \" \" + json[\"Mahkeme Günü\"] + \" \" + json[\"Mahkeme Ayı\"] + \" \" + json[\"Mahkeme Yılı\"] + \" \" + json[\"Suç\"] + \" \" + json[\"Hüküm\"] + \" \" + json[\"Dava Türü\"] + \" \" + json[\"ictihat\"] + \" \"\n",
        "  temptext = temptext.lower()\n",
        "  tokens = nltk.word_tokenize(temptext)\n",
        "  for i in tokens:\n",
        "      if i in chunk:\n",
        "        tokens.remove(i)\n",
        "  for i in tokens:\n",
        "     if i not in stop_words:\n",
        "        tokens_no_stop_words.append(i)\n",
        "\n",
        "# Splitting words into test and training sets for word based n-gram model.\n",
        "training_size_for_word_based = int(len(tokens_no_stop_words) * 0.8)\n",
        "training_set_for_word_based = tokens_no_stop_words[:training_size_for_word_based]\n",
        "test_set_for_word_based = tokens_no_stop_words[training_size_for_word_based:]\n",
        "\n",
        "#Forming 2-grams and training model.\n",
        "training_ngrams_for_word_based, vocab = padded_everygram_pipeline(2, training_set_for_word_based)\n",
        "word_based_model = Laplace(1)\n",
        "word_based_model.fit(training_ngrams_for_word_based,vocab)\n",
        "\n",
        "#Testing word based n-gram model\n",
        "test_ngrams_for_word_based = ngrams(test_set_for_word_based,2)\n",
        "perplexity = word_based_model.perplexity(test_ngrams_for_word_based)\n",
        "print(\"Perplexity for word based model: \", perplexity)\n",
        "\n",
        "# Tokenizing words into characters.\n",
        "characters_from_tokenized_no_stop_words = []\n",
        "for i in tokens_no_stop_words:\n",
        "  for j in i:\n",
        "    characters_from_tokenized_no_stop_words.extend(j)\n",
        "\n",
        "#Splitting characters into training and test sets.\n",
        "training_size_for_character_based = int(len(characters_from_tokenized_no_stop_words) * 0.8)\n",
        "training_set_for_character_based = characters_from_tokenized_no_stop_words[:training_size_for_character_based]\n",
        "test_set_for_character_based = characters_from_tokenized_no_stop_words[training_size_for_character_based:]\n",
        "\n",
        "# Forming 2-grams for character based model and training it.\n",
        "training_ngrams_for_character_based, vocab2 = padded_everygram_pipeline(2,training_set_for_character_based)\n",
        "character_based_model = Laplace(1)\n",
        "character_based_model.fit(training_ngrams_for_character_based,vocab2)\n",
        "\n",
        "# Testing character based model.\n",
        "test_ngrams_for_character_based = ngrams(test_set_for_character_based,2)\n",
        "perplexity = word_based_model.perplexity(test_ngrams_for_character_based)\n",
        "print(\"Perplexity for character based model: \", perplexity)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import random\n",
        "from nltk.util import ngrams\n",
        "# Reading test files from test folder. This folder should be in google drive and test jsons should be placed in it.\n",
        "dir_for_test = \"/content/drive/MyDrive/test\"\n",
        "jsons_for_test = []\n",
        "for filename in os.listdir(dir_for_test):\n",
        "  if filename.endswith(\".json\"):\n",
        "    jsonpath = os.path.join(dir_for_test, filename)\n",
        "    with open(jsonpath, \"r\") as jsonfile:\n",
        "            myjsonfile = json.load(jsonfile)\n",
        "            jsons_for_test.append(myjsonfile)\n",
        "tokens_for_test_no_stop_words = []\n",
        "stop_words_for_test = nltk.corpus.stopwords.words('turkish')\n",
        "chunk_for_test = [\".\",\",\",\"/\",\"[\",\"]\",\":\",\";\",\"...\"]\n",
        "\n",
        "#Reading words from json files and tokenizing them. Then removing expressions like \".\" , \" /\" etc and stop words.\n",
        "for json in jsons_for_test:\n",
        "  temptext = \"\"\n",
        "  for i in json[\"vurgular\"]:\n",
        "    temptext += i + \" \"\n",
        "  temptext += json[\"Mahkemesi\"] + \" \" + json[\"Dairesi\"] + \" \" + json[\"Esas\"] + \" \" + json[\"Birinci Mahkemesi\"] + \" \" + json[\"Karar\"] + \" \" + json[\"Mahkeme Günü\"] + \" \" + json[\"Mahkeme Ayı\"] + \" \" + json[\"Mahkeme Yılı\"] + \" \" + json[\"Suç\"] + \" \" + json[\"Hüküm\"] + \" \" + json[\"Dava Türü\"] + \" \" + json[\"ictihat\"] + \" \"\n",
        "  temptext = temptext.lower()\n",
        "  tokens = nltk.word_tokenize(temptext)\n",
        "  for i in tokens:\n",
        "      if i in chunk_for_test:\n",
        "        tokens.remove(i)\n",
        "  for i in tokens:\n",
        "     if i not in stop_words:\n",
        "        tokens_for_test_no_stop_words.append(i)\n",
        "#Forming 2-gram words for testing word based model.\n",
        "test_for_word_based = ngrams(tokens_for_test_no_stop_words,2)\n",
        "perplexity_for_word_based = word_based_model.perplexity(test_for_word_based)\n",
        "print(\"Perplexity test result for word based: \", perplexity_for_word_based)\n",
        "\n",
        "#Splitting words into characters.\n",
        "charactecters_for_test = []\n",
        "for i in tokens_for_test_no_stop_words:\n",
        "  for j in i:\n",
        "    charactecters_for_test.extend(j)\n",
        "\n",
        "#Forming 2-gram characters and testing character based model.\n",
        "test_for_character_based = ngrams(charactecters_for_test,2)\n",
        "perplexity_for_character_based = character_based_model.perplexity(test_for_character_based)\n",
        "print(\"Perplexity test result for character based model: \", perplexity)\n"
      ],
      "metadata": {
        "id": "MjEyUSVwZ3K0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}